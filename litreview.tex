\documentclass[a4paper,11pt]{article}

% set up sensible margins (same as for cssethesis)
\usepackage[paper=a4paper,left=30mm,width=150mm,top=25mm,bottom=25mm]{geometry}
\usepackage{setspace}               % This is used in the title page
\usepackage{graphicx}               % This is used to load the crest in the title page
\usepackage{poltakmacros}           % Personal macros included in file 'poltakmacros.sty'
\usepackage{enumitem}               % For nested enum lists
\usepackage{titlesec}
\usepackage[font={small}]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{url}

% Settings for titlesec package to have titles down to level of 4
\setcounter{secnumdepth}{4}

\author{Jonathan Poltak Samosir}
\title{Honours Research Proposal}

\begin{document}

% Set up a title page
\thispagestyle{empty} % no page number on very first page
% Use roman numerals for page numbers initially
\renewcommand{\thepage}{\roman{page}}

\begin{spacing}{1.5}
\begin{center}
{\Large \bfseries
Clayton School of Information Technology\\
Monash University}

\vspace*{30mm}

\includegraphics[width=5cm]{img/MonashCrest.pdf}

\vspace*{15mm}

{\large \bfseries
Honours Literature Review --- Semester 2, 2014
}

\vspace*{10mm}

{\LARGE \bfseries
A study of the Hadoop ecosystem for pipelined realtime data stream processing
}

\vspace*{20mm}

{\large \bfseries
Jonathan Poltak Samosir

[2271 3603]

\vspace*{20mm}

Supervisors: \parbox[t]{50mm}{\mbox{Dr Maria Indrawan-Santiago}\\Dr Pari Delir Haghighi}
}

\end{center}
\end{spacing}

\newpage

\tableofcontents

\newpage
% Now reset page number counter,and switch to arabic numerals for remaining page numbers
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}


% Start of content

\section{Introduction} % (fold)
\label{sec:introduction}

The realtime processing of big data is of great importance to both academia and industry. Advancements and progress in
modern society can be directly attributed back to data. The value of data has become more apparent, and data has become
a sort of currency for the information economy~\cite{st2009examining}. Hence, those in society who realised the value of
data early immense power over the entire economy and thus society overall~\cite{lievesley1993increasing}.
From seemingly inconsequential gains at the macro level, such as the ability to more
accurately predict the rise and fall of airline tickets~\cite{darlin2006airfares}, to those of utmost importance for
society as a whole, such as predicting and tracking the spread of the Swine Flu Pandemic in 2009 more accurately that
the United States Centers for Disease Control and Prevention could~\cite{ritterman2009using}~\cite{mayer2013big}. It is
example applications of big data processing like these that have been recognised by academics and organisations in
industry alike, with the last decade seeing a major shift in research and development into new methods for the handling
and processing of big data.

This paper will give a background on the types and classes of big data, as well as the various methods employed to
process those given classes of data. We will more specifically focussing on the methods that are involved with the
analysis and processing of realtime data streams, as opposed to the batch processing of big data. This paper will look
into detail at previous work that has been done in the field of big data, specifically those works that have had a
greater influence on the field  as a whole. This includes both works looking specifically at the processing of streaming
data, and works involving processed big data in batch mode, given that batch mode processing arguably led onto the
current hot-topic of realtime stream processing.

This paper will be structured in two main sections. In~\sectref{sec:big_data_types_background}, an overview of the different
classes and types of big data will be presented. This includes an overview of the big data classes presented through others' findings
as well as our own proposed classes for big data, based on the criticisms of those prior findings. In~\sectref{sec:big_data_processing_background},
an overview will be given of the major open-source big data processing systems. A special emphasis will be given on data stream processing
systems (DSPSs), given that the main area of this research is focusing on realtime data processing, or data stream processing.

\sectref{sec:relationships_between_big_data_classes_and_big_data_processing} will then give a discussion relating to future work
we have planned to form data processing recommendations based on the classification of specific data classes. All of the
sections will then be summarised in the conclusion in~\sectref{sec:conclusion}.

As a an outcome of this paper, we will identify a gap in previous research and development in the big data processing
field, upon which our future work will attempt to work towards filling.

% section introduction (end)


\section{Data types and characteristics background} % (fold)
\label{sec:big_data_types_background}

\subsection{Velocity, variety, volume, and veracity} % (fold)
\label{sub:four_v}

Data, and more specifically, big data, are often characterised into what is known as the ``four V's''~\cite{wang2014bigdatabench}.
These can be thought of as different ``dimensions'' of big data, and can be summarised as follows~\cite{dong2013big}:

\begin{itemize}
  \item \emph{Velocity:} The rate at which data is being collected and made available to the data consumers.
  \item \emph{Variety:} The heterogeneity of data. Big data often exhibits substantial variations in both the structural
  level and the instance level (representations of real-world entities). This is often highlighted by data systems that
  depend on acquiring of data from a number of non-conforming, and sometimes unrelated, data sources.
  \item \emph{Volume:} The amount of data that is obtained by the data consumer from the data source/s.
  \item \emph{Veracity:} The quality, in terms of accuracy, coverage, and timeliness, of data that is consumed from
  the data source/s. Veracity of data can widely differ between sources.
\end{itemize}

While the four V's are often described in terms of big data, they can also apply to more traditional data
warehousing and processing in general, albeit on a far smaller scale. In the domain of big data processing, data will
exhibit signs of high velocity, variety, and volume~\cite{beyer2011gartner}, and hence the veracity of the data may also fluctuate. Meanwhile,
in more traditional data processing, the scope may be limited, especially in terms of factors such as variety and, as a
consequence, there is less need of an emphasis on veracity due to limited variety in data sources.

As will be made clear in the following sections, a lot of the identified classes and characteristics of data directly
relate back to these four V's, whether or not it was intentional by the original authors. These can be considered the
underlying features of many characteristics of data, both in the sense of big data and traditional data.

% subsection velocity_variety_volume_and_veracity (end)


\subsection{Classification of data} % (fold)
\label{sub:data_classification}

Data, in general, can be categorised into a number of different classes or types. In this paper, we will define the
concept of a data class to mean the same as the terms of ``data type'', ``data category'', or ``data format'', as all
terms were often used interchangeably in other literature.

Each class of data can be further defined and categorised via the characteristics they exhibit. Furthermore, these
characteristics exhibited by data classes can be exploited and it is often possible to optimise the processing of each
class of data by processing it using a specific method depending on those characteristics.%TODO cite
To give an example
of this, data that is expected to have highly iterative processing applied to it would benefit from a data processor
that does not have to unnecessarily write to disk after every single iteration. The elimination of this I/O overhead is
an example of the optimisations that could be applied to the overall process from correctly identifying the data class
beforehand, and processing it accordingly.

Furthermore, particular classes of data are generally only found in particular applications or use cases of data
processing.%TODO cite
As this is the case, it narrows down the amount of classification needed, depending on the application that
is being looked at. This will be elaborated on in later parts of this section.

There is no concrete, universally accepted standard for the classification of data. While the study of big data processing
could arguably be considered still in its infancy (or at least temperamental toddler stage), data handling and processing
in general is relatively mature. From preliminary research on looking at past work and literature in this area,
it must be noted that there is a significant lack of research on the classification of data.

The literature that
will be reviewed in this section is often not wholly focused on the idea of data classification, hence data classification
is presented relative to whatever the overall topic of the literature is on. This is important to note, as one attempt
at data classification may not be appropriate under a different context. This also explains the large variation in different
classification attempts, although we will also highlight the recurring similarities between different data classification
literature.



\subsubsection{Characteristics of data, from Mysore et al.} % (fold)
\label{ssub:data_charact}

The main piece of literature that this section sources is a white paper from IBM Architects Mysore, Khupat, and Jain,
published by IBM in 2013~\cite{ibm_big_2013}. The white paper is targeted towards beginners in the area of big data
processing; much like the set of recommendations that we intend to produce from this research project. The paper looks
at identifying the different data classes, or ``formats'' as they were labelled in the paper, that are commonly
encountered in big data. For each of these formats, what was identified was the underlying characteristics of the data,
and it was noted that the type of processing needed would be dependent on those characteristics.

The characteristics of data, as put forward by Mysore et al., in~\cite{ibm_big_2013}, include the following:

\paragraph{Analysis type:}

\begin{itemize}
  \item Whether or not the data would be processed/analysed in realtime, or batched for later processing.
  \item Often this data class characteristic is dependent on the application of the data (\eg{}The processing of social
  media data for the analysis of currently occurring events would want to be processed in realtime, regardless of the
  type of data that is involved).
\end{itemize}

\paragraph{Processing methodology:}

\begin{itemize}
  \item This characteristic involves the approach used when processing the data.
  \item Some examples of different processing methodologies include: predictive processing, analytical, ad-hoc queries,
  and reporting.
  \item Often the processing methodology for a particular class is determined by the business requirements or application
  of the data.
  \item Depending on the processing methodology used, many different combinations of big data technologies can be used.
\end{itemize}

\paragraph{Data frequency and size:}

\begin{itemize}
  \item The amount of data expected to arrive to the processing system, along with the speed and regularity of the incoming data.
  \item Knowing this characteristic beforehand can determine the methods for data storage and preprocessing, if needed.
  \item Examples of data frequency includes: on-demand data (social media), continuous/realtime (weather data, transactions),
  time-series (email).
  \item Considering the four V's, the characteristic of data frequency and size directly relates back to velocity and volume.
\end{itemize}

\paragraph{Content format:}

\begin{itemize}
  \item This characteristic relates back to the structure of the underlying data.
  \item Examples of data content format include: structured (JSON, XML), unstructured (human-readable literature),
  semi-structured (email).
\end{itemize}

\paragraph{Data source:}

\begin{itemize}
  \item This characteristic relates back to where the data originated from.
  \item As discussed previously in~\sectref{sub:four_v}, the origin of data can have a great effect on whether or not
  that data is usable, as data often varies greatly, especially when many different sources are used which may or may
  not conform to a specific content format.
  \item Another thing that is dependent on the data source is whether or not the data can be trusted.
  \item Considering the four V's, the characteristic of data source directly relates back to veracity and variety.
\end{itemize}

% subsubsection data_charact (end)



\subsubsection{Classes of data, from Mysore et al.} % (fold)
\label{ssub:data_classification}

The following table highlights the different classes of data put forward by Mysore, et al., in~\cite{ibm_big_2013}.
The table organises each class, along with giving a brief explanation of the class. Furthermore, each class is related
back to the previously explained characteristics in an attempt to show the connections between class and underlying
characteristics.

\hspace*{-3cm}
\begingroup
\fontsize{8pt}{10pt}\selectfont
\begin{tabular}{ | p{1.5cm} | p{8cm} | p{8cm} | }
  \hline
  \textbf{Data class}          &  \textbf{Explanation} & \textbf{Characteristics}   \\ \hline

  Machine generated data
  &
  \begin{itemize}
    \item Data that is automatically generated as a by-product of some interaction with a machine.
    \item While Mysore et al.\ present this as being a distinct class in itself, it could be argued that this class
    is an umbrella class which many other data classes presented in their paper fall under. This will be touched upon
    further in later sections.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured data (JSON, XML).
    \item Frequency of data varies depending on application.
  \end{itemize}
  \\ \hline

  Web and social data
  &
  \begin{itemize}
    \item Data that is automatically generated through use of the Internet or social media, such as Facebook or Twitter.
  \end{itemize}
  &
  \begin{itemize}
    \item Unstructured text (long: blogs, short: microblogs, Facebook).
    \item Miscellaneous multimedia (video, image, audio).
    \item On-demand frequency.
    \item Can be continuous feed of data in cases such as Twitter.
  \end{itemize}
  \\ \hline

  Transaction data
  &
  \begin{itemize}
    \item Data that is automatically generated as a by-product of transactions, such as money transactions or otherwise.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured text (JSON, XML, logs).
    \item Continuous feed.
  \end{itemize}
  \\ \hline

  Human generated data
  &
  \begin{itemize}
    \item Data that is solely produced by humans.
    \item Examples of human generated data, as it is defined here, include such things as music, literature, recordings,
    and emails.
  \end{itemize}
  &
  \begin{itemize}
    \item Unstructured text (mail, literature).
    \item Miscellaneous multimedia (audio, video, images).
    \item Semi-structured text (email, online messaging services).
    \item On-demand frequency.
  \end{itemize}
  \\ \hline

  Biometrics data
  &
  \begin{itemize}
    \item Data that relates to human bioinformatics.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured data.
    \item On-demand frequency.
    \item Continuous feeds of data in cases such as persistent health monitoring sensors (\ie{}hospital patients).
  \end{itemize}
  \\ \hline

\end{tabular}
\endgroup
\hspace*{5cm}

The classes and characteristics of data presented by Mysore et al., in~\cite{ibm_big_2013}, are highly oriented towards
industry and business users, coming from an IBM-published paper. While this is not an issue as such, as noted earlier
in this section, these characteristics and data classes are defined within the domain relevant to this paper. As such,
they may not be as relevant or appropriate for usage in other, non-business domains or even business domains with a
different focus on data.

% subsubsection data_classification (end)


\subsubsection{Characteristics of data, from Chen et al.} % (fold)
\label{ssub:characteristics_of_data_from_chen_et_al_}

The second paper sourced is a paper from Chen, Chiang, and Storey, focusing on the impact of big data in the field of
business intelligence and analytics~\cite{chen2012business}.

% subsubsection characteristics_of_data_from_chen_et_al_ (end)


\subsubsection{Characteristics of data, from G\'eczy} % (fold)
\label{ssub:characteristics_of_data_from_ge_czy}

Coming away from the business point-of-view, G\'eczy attempts to characterise data, and more specifically big data,  in
a more generic way~\cite{geczy_big_2014}. He uses what he labels as ``aspects'' to determine what he believes to be the
deciding characteristics of data, in terms of the way they should be processed and also simply their intrinsic traits.

G\'eczy uses the following aspects to determine the different intrinsic characteristics of data:

\paragraph{Sensitivity:}

\begin{itemize}
  \item Relates to whether or not given data contains sensitive information, \ie{}personally identifiable information,
  confidential information, etc.
  \item The sentivity of the data determines the requirements relating to how it should be handled.
  \item Often it is either a legal requirement, or in the owners' interest, to keep protected the handled data deemed sensitive.
\end{itemize}


\paragraph{Diversity:}

\begin{itemize}
  \item Relates to the range of different data elements present within the data.
  \item The example given explains the ability of smart phones to produce highly diverse data; \eg{}audio, video, location
  data, gyroscopic data, etc.
  \item Having high diversity in data can both be beneficial and detrimental; diversity can add factors of complexity,
  although also makes for a more rich dataset.
  \item Note that this data characteristic relates directly back to the \emph{Variety} dimension, of the four V's.
\end{itemize}


\paragraph{Quality:}

\begin{itemize}
  \item Quality characteristics of data are defined to be features that affect data quality; \eg{}completeness, accuracy,
  timeliness.
  \item Often the quality of data may be subject to the qualitative metrics of an organisation, or predefined standards.
  \item The quality of data relates back to the \emph{Veracity} dimension, of the four V's.
\end{itemize}


\paragraph{Volume:}

\begin{itemize}
  \item Volume refers to the size of data in terms of its basic forms of measurement, bits and bytes.
  \item Volume is an important characteristic to take into consideration when it comes to determining the type of
  processing needed.
  \item Volume, as the name suggests, directly relates back to the Volume dimension of the four V's.
\end{itemize}


\paragraph{Speed:}

\begin{itemize}
  \item Data speed refers to the inflow and outflow speeds; inflow being the data that is being acquired, while outflow
  being the data leaving the system (often results of computations).
  \item Different classes of data often require different data speeds. \eg{}audio is often streamed at a far lesser speed
  than video, due to the relatively low amount of data in audio when compared with video.
\end{itemize}


\paragraph{Structure:}

\begin{itemize}
  \item Structure relates to whether data are in structured or unstructured formats.
  \item Generally unstructured data is more suitable for human consumption, such as literature or music.
  \item Structured data is usually structured in such a way that it is easily able to be parsed by an algorithm, often
  automated by computers.
  \item The structure of data directly relates to the difficulty of processing that data, as unstructured data usually
  will need some pre-processing or artificial intelligence to process.
\end{itemize}

G\'eczy later goes on to talk about the aspects of data that relate to data processing. This will be looked at further
in~\sectref{} %TODO

Overall, G\'eczy looks at data characteristics, not from any particular perspective, but from one that attempts to capture
the interests and be relevant to a number of disciplines. This impartiality is a nice refreshment from most other literature
available on the topic, which have been shown to have been looking at data classification from a certain point-of-view.
However, this should not be misinterpreted as a criticism of the previous literature. It is simply that the classes of
data identified in other author's literature was more appropriate for the topic on which the rest of their research was
focussed on. Hence, the way they treated data changed accordingly. The paper presented by G\'eczy, simply titled
\emph{BIG DATA CHARACTERISTICS}, was focussed on nothing other than characteristics of data, hence there was no reason
to attempt to classify those characteristics based on any other domain-related biases. This is the important difference
to note, especially as we revisit this paper in~\sectref{} %TODO.

% subsubsection characteristics_of_data_from_ge_czy (end)


\subsubsection{Criticisms of previously presented models} % (fold)
\label{ssub:criticisms_of_previously_presented_models}

% subsubsection criticisms_of_previously_presented_models (end)

% subsection data_classification (end)

% section big_data_types_background (end)


\section{Big data processing background} % (fold)
\label{sec:big_data_processing_background}

Much work has been done in the area of big data processing. As discussed in

% section big_data_processing_background (end)


\section{Relationships between big data classes and big data processing} % (fold)
\label{sec:relationships_between_big_data_classes_and_big_data_processing}

% section relationships_between_big_data_classes_and_big_data_processing (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\newpage

\bibliographystyle{acm}
\bibliography{litreview.bib}

\end{document}

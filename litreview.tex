\documentclass[a4paper,11pt]{article}

% set up sensible margins (same as for cssethesis)
\usepackage[paper=a4paper,left=30mm,width=150mm,top=25mm,bottom=25mm]{geometry}
\usepackage{setspace}               % This is used in the title page
\usepackage{graphicx}               % This is used to load the crest in the title page
\usepackage{poltakmacros}           % Personal macros included in file 'poltakmacros.sty'
\usepackage{enumitem}               % For nested enum lists
\usepackage{titlesec}
\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\restoreparindent
\usepackage[font={small}]{caption}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{url}

% Settings for titlesec package to have titles down to level of 4
\setcounter{secnumdepth}{4}

\author{Jonathan Poltak Samosir}
\title{Honours Research Proposal}

\begin{document}

% Set up a title page
\thispagestyle{empty} % no page number on very first page
% Use roman numerals for page numbers initially
\renewcommand{\thepage}{\roman{page}}

\begin{spacing}{1.5}
\begin{center}
{\Large \bfseries
Clayton School of Information Technology\\
Monash University}

\vspace*{30mm}

\includegraphics[width=5cm]{img/MonashCrest.pdf}

\vspace*{15mm}

{\large \bfseries
Honours Literature Review --- Semester 2, 2014
}

\vspace*{10mm}

{\LARGE \bfseries
A study of the Hadoop ecosystem for pipelined realtime data stream processing
}

\vspace*{20mm}

{\large \bfseries
Jonathan Poltak Samosir

[2271 3603]

\vspace*{20mm}

Supervisors: \parbox[t]{50mm}{\mbox{Dr Maria Indrawan-Santiago}\\Dr Pari Delir Haghighi}
}

\end{center}
\end{spacing}

\newpage

\tableofcontents

\newpage
% Now reset page number counter,and switch to arabic numerals for remaining page numbers
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}


% Start of content

\section{Introduction} % (fold)
\label{sec:introduction}

The realtime processing of big data is of great importance to both academia and industry. Advancements and progress in
modern society can be directly attributed back to data. The value of data has become more apparent, and data has become
a sort of currency for the information economy~\cite{st2009examining}. Hence, those in society who realised the value of
data early hold immense power over the entire economy, and in turn society, overall~\cite{lievesley1993increasing}.
From seemingly inconsequential gains at the macro level, such as the ability to more
accurately predict the rise and fall of airline tickets~\cite{darlin2006airfares}, to those of utmost importance for
society as a whole, such as predicting and tracking the spread of the Swine Flu Pandemic in 2009 more accurately that
the United States Centers for Disease Control and Prevention could~\cite{ritterman2009using}~\cite{mayer2013big}. It is
example applications of big data processing like these that have been recognised by academics and organisations in
industry alike, with the last decade seeing a major shift in research and development into new methods for the handling
and processing of big data.

This review will give a background on the types and classes of big data, as well as the various methods employed to
process those given classes of data. We will more specifically focussing on the methods that are involved with the
analysis and processing of realtime data streams, as opposed to the batch processing of big data. This review will look
into detail at previous work that has been done in the field of big data, specifically those works that have had a
greater influence on the field  as a whole. This includes both works looking specifically at the processing of streaming
data, and works involving processed big data in batch mode, given that batch mode processing arguably led onto the
current hot-topic of realtime stream processing.

This review will be structured in three main sections. In~\sectref{sec:big_data_types_background}, an overview of the different
classes and types of big data will be presented. This includes an overview of the big data classes presented through others' findings
as well as our own proposed classes for big data, based on the criticisms of those prior findings. In~\sectref{sec:big_data_processing_background},
an overview will be given of the major open-source big data processing systems. A special emphasis will be given on data stream processing
systems (DSPSs), given that the main area of this research is focusing on realtime data processing, or data stream processing.
In~\sectref{sec:analysis_and_discussion} an analysis and discussion will be given on the content covered from the
relevant literature. All of the sections will then be summarised in the conclusion in~\sectref{sec:conclusion}.

% section introduction (end)


\section{Data types and characteristics background} % (fold)
\label{sec:big_data_types_background}

\subsection{Velocity, variety, volume, and veracity} % (fold)
\label{sub:four_v}

Data, and more specifically, big data, are often characterised into what is known as the ``four V's''~\cite{wang2014bigdatabench}.
These can be thought of as different ``dimensions'' of big data, and can be summarised as follows~\cite{dong2013big}:

\begin{itemize}
  \item \emph{Velocity:} The rate at which data is being collected and made available to the data consumers.
  \item \emph{Variety:} The heterogeneity of data. Big data often exhibits substantial variations in both the structural
  level and the instance level (representations of real-world entities). This is often highlighted by data systems that
  depend on acquiring of data from a number of non-conforming, and sometimes unrelated, data sources.
  \item \emph{Volume:} The amount of data that is obtained by the data consumer from the data source/s.
  \item \emph{Veracity:} The quality, in terms of accuracy, coverage, and timeliness, of data that is consumed from
  the data source/s. Veracity of data can widely differ between sources.
\end{itemize}

While the four V's are often described in terms of big data, they can also apply to more traditional data
warehousing and processing in general, albeit on a far smaller scale. In the domain of big data processing, data will
exhibit signs of high velocity, variety, and volume~\cite{beyer2011gartner}, and hence the veracity of the data may also fluctuate. Meanwhile,
in more traditional data processing, the scope may be limited, especially in terms of factors such as variety and, as a
consequence, there is less need of an emphasis on veracity due to limited variety in data sources.

As will be made clear in the following sections, a lot of the identified classes and characteristics of data directly
relate back to these four V's. These can be considered the underlying features of many characteristics of data, both in
the sense of big data and traditional data.

% subsection velocity_variety_volume_and_veracity (end)


\subsection{Classification of data} % (fold)
\label{sub:data_classification}

Data, in general, can be categorised into a number of different classes or types. We define the concept of a data class
to mean the same as the terms of ``data type'', ``data category'', or ``data format'', as all terms were often used
interchangeably in other literature.

Each class of data can be further defined and categorised via the characteristics they exhibit. Furthermore, these
characteristics exhibited by data classes can be exploited and it is often possible to optimise the processing of each
class of data by processing it using a specific method depending on those characteristics.

To give an example of this, data that is expected to have highly iterative processing applied to it would benefit from a
data processor that does not have to unnecessarily write to disk after every single iteration. The elimination of this
I/O overhead is an example of the optimisations that could be applied to the overall process from correctly identifying
the data class beforehand, and processing it accordingly.

Furthermore, particular classes of data are generally only found in particular applications or use cases of data
processing. As this is the case, it narrows down the amount of classification needed, depending on the application that
is being looked at. This will be elaborated on in later parts of this section.

There is no concrete, universally accepted standard for the classification of data. While the study of big data
processing can arguably be considered still in its infancy, data handling and processing in general is relatively
mature. From preliminary research on looking at past work and literature in this area, it must be noted that there is a
significant lack of research on the classification of data.

The literature that will be reviewed in this section is often not wholly focused on the idea of data classification,
hence data classification is presented relative to whatever the overall topic of the literature is on. This is important
to note, as one attempt at data classification may not be appropriate under a different context. This also explains the
large variation in different classification attempts, although we will also highlight the recurring similarities between
different data classification literature.


\subsubsection{IBM's classification of big data}
\label{ssub:data_charact}

This section looks at a classification of big data types, along with the key data characteristics, proposed by IBM
Architects Mysore, Khupat, and Jain, published by IBM in 2013~\cite{ibm_big_2013}. The content is targeted towards
beginners in the area of big data processing; much like the set of recommendations that we intend to produce from this
research project. The different data classes, or ``formats'' as they were labelled, that are commonly encountered in big
data are identified. For each of these formats, the underlying characteristics of the data was discussed, and it was
noted that the type of processing needed would be dependent on those characteristics.

\noindent The characteristics of data, as put forward by Mysore et al., in~\cite{ibm_big_2013}, include the following:

\noindent \textbf{Analysis type -} Whether or not the data would be processed/analysed in realtime, or batched for later
processing. Often this data class characteristic is dependent on the application of the data, \eg{}the processing of
social media data for the analysis of currently occurring events would want to be processed in realtime, regardless of
the type of data that is involved.

\noindent \textbf{Processing methodology -} This characteristic involves the approach used when processing the data. Some examples
of different processing methodologies include: predictive processing, analytical, ad-hoc queries, and reporting. Often
the processing methodology for a particular class is determined by the business requirements or application of the
data. Depending on the processing methodology used, many different combinations of big data technologies can be used.

\noindent \textbf{Data frequency and size -} The amount of data expected to arrive to the processing system, along with the speed
and regularity of the incoming data. Knowing this characteristic beforehand can determine the methods for data storage
and preprocessing, if needed. Examples of data frequency includes: on-demand data (social media), continuous/realtime
(weather data, transactions), time-series (email). Considering the four V's, the characteristic of data frequency and
size directly relates back to velocity and volume.

\noindent \textbf{Content format -} This characteristic relates back to the structure of the underlying data. Examples of data
content format include: structured (JSON, XML), unstructured (human-readable literature), semi-structured (email).

\noindent \textbf{Data source -} This characteristic relates back to where the data originated from. As discussed previously
in~\sectref{sub:four_v}, the origin of data can have a great effect on whether or not that data is usable, as data often
varies greatly, especially when many different sources are used which may or may  not conform to a specific content
format. Another thing that is dependent on the data source is whether or not the data can be trusted. Considering the
four V's, the characteristic of data source directly relates back to veracity and variety.

The table, found in \textbf{Appendix~\ref{app:ibm_data_classes}}, highlights the different classes of data put forward
by Mysore, et al., in~\cite{ibm_big_2013}. The table organises each class, along with giving a brief explanation of the
class. Furthermore, each class is related back to the previously explained characteristics in an attempt to show the
connections between class and underlying characteristics.

The classes and characteristics of data presented by Mysore et al., in~\cite{ibm_big_2013}, are highly oriented towards
industry and business users. While this is not an issue as such, as noted earlier in this section, these characteristics
and data classes may not be as relevant or appropriate for usage in other non-business domains, or even business domains
with a different focus on data.

% subsubsection data_classification (end)


\subsubsection{Characteristics of data, from Chen et al.} % (fold)
\label{ssub:characteristics_of_data_from_chen_et_al_}

The second paper sourced is a paper from Chen, Chiang, and Storey, focusing on the impact of big data in the field of
business intelligence and analytics~\cite{chen2012business}. Similarly to the paper looked at in~\sectref{ssub:data_charact},
there is an emphasis on data classes and how they relate to the area of business and organisations. However, this paper
has more of an explicit focus on business, being in published in the area of business intelligence and analytics (BI\&A).
BI\&A in itself is a highly data driven field, where data is gathered and analysed to help make informed business
decisions~\cite{watson2009tutorial}.

In the paper, Chen et al., discuss the evolution of the field of BI\&A, which they categorise into three distinct stages.
BI\&A 1.0, being the first of the three, focuses on more traditional data processing and analysis. This includes
highly structured and relational data. BI\&A 2.0 involves more unstructured, web-based content with the rise of ``Web 2.0''
technologies, including social networks and opinion pieces, such as blogs. BI\&A 3.0 looks at more mobile and sensor-based
data. This data differentiates itself mostly to do with characteristics such as location-based data and data that is
highly context dependent.

Chen et al., elaborate on these different stages of BI\&A evolution through showing the major BI\&A applications for the
previously mentioned evolutionary stages. For each of the BI\&A applications presented, they attempt to show the classes
of data which are important for the particular application, and subsequently the characteristics associated which each
class. The classes and characteristics of data, shown by Chen et al., in relation to BI\&A will be presented here. They
will be presented in terms of the BI\&A application of which they are categorised under.

\paragraph{E-Commerce and Market Intelligence:\\}

Types of data include:

\begin{itemize}
  \item Website logs and analytics data.
  \item User activity logs for e-commerce websites.
  \item User transaction records.
  \item User-generated content, such as reviews, feedback.
\end{itemize}

Characteristics of the data include:

\begin{itemize}
  \item Structured web-based data (transactions records, logs, network information).
  \item Unstructured user-generated content (reviews, feedback).
\end{itemize}


\paragraph{E-Government and Politics 2.0:\\}

Types of data include:

\begin{itemize}
  \item Government information, such as statistics.
  \item Rules and regulations.
  \item Citizen-generated content, such as feedback, comments, and requests.
\end{itemize}

Characteristics of the data include:

\begin{itemize}
  \item Fragmented data sources (think high data variety).
  \item Unstructured data (citizen-generated content).
  \item Rich textual content.
\end{itemize}


\paragraph{Science \& Technology:\\}

Types of data include:

\begin{itemize}
  \item Machine-generated data from tools and instruments.
  \item Sensor data.
  \item Network data.
\end{itemize}

Types of characteristics include:

\begin{itemize}
  \item High velocity data collection from instruments and tools and sensors.
  \item Structured data, often formatted in uncommon structures.
\end{itemize}


\paragraph{Smart Health and Wellbeing:\\}

Types of data include:

\begin{itemize}
  \item Genomics and sequence data (DNA sequences).
  \item Electronic health records.
  \item Health and patient social media.
\end{itemize}

Types of characteristics include:

\begin{itemize}
  \item Varying, but interrelated, data.
  \item Data specific to individual patients.
\end{itemize}


\paragraph{Security and Public Safety:\\}

Types of data include:

\begin{itemize}
  \item Criminal record data.
  \item Statistical data (crime maps).
  \item Media content relating to crime (news articles).
  \item Cyber-crime data (computer viruses, botnet data).
\end{itemize}

Types of characteristics include:

\begin{itemize}
  \item Highly sensitive information (identity data).
  \item Incomplete and deceptive content (speculative media content).
  \item Multilingual content.
\end{itemize}

As can be seen from the data classes and characteristics identified by Chen et al., in~\cite{chen2012business}, there is
a far greater variation to those previously presented by Mysore et al., in~\cite{bifet_mining_2013}. As explained earlier,
this is mainly because of the more domain specific content of this piece of literature, while the paper from Mysore et
al., while still having underlying tones of business and industry, had a less explicit focus on their particular domain.

% subsubsection characteristics_of_data_from_chen_et_al_ (end)


\subsubsection{Characteristics of data, from G\'eczy} % (fold)
\label{ssub:characteristics_of_data_from_ge_czy}

Coming away from the business point-of-view, G\'eczy attempts to characterise data, and more specifically big data,  in
a more generic way~\cite{geczy_big_2014}. He uses what he labels as ``aspects'' to determine what he believes to be the
deciding characteristics of data, in terms of the way they should be processed and also simply their intrinsic traits.

G\'eczy uses the following aspects to determine the different intrinsic characteristics of data:

\paragraph{Sensitivity:}

\begin{itemize}
  \item Relates to whether or not given data contains sensitive information, \ie{}personally identifiable information,
  confidential information, etc.
  \item The sentivity of the data determines the requirements relating to how it should be handled.
  \item Often it is either a legal requirement, or in the owners' interest, to keep protected the handled data deemed sensitive.
\end{itemize}


\paragraph{Diversity:}

\begin{itemize}
  \item Relates to the range of different data elements present within the data.
  \item The example given explains the ability of smart phones to produce highly diverse data; \eg{}audio, video, location
  data, gyroscopic data, etc.
  \item Having high diversity in data can both be beneficial and detrimental; diversity can add factors of complexity,
  although also makes for a more rich dataset.
  \item Note that this data characteristic relates directly back to the \emph{Variety} dimension, of the four V's.
\end{itemize}


\paragraph{Quality:}

\begin{itemize}
  \item Quality characteristics of data are defined to be features that affect data quality; \eg{}completeness, accuracy,
  timeliness.
  \item Often the quality of data may be subject to the qualitative metrics of an organisation, or predefined standards.
  \item The quality of data relates back to the \emph{Veracity} dimension, of the four V's.
\end{itemize}


\paragraph{Volume:}

\begin{itemize}
  \item Volume refers to the size of data in terms of its basic forms of measurement, bits and bytes.
  \item Volume is an important characteristic to take into consideration when it comes to determining the type of
  processing needed.
  \item Volume, as the name suggests, directly relates back to the Volume dimension of the four V's.
\end{itemize}


\paragraph{Speed:}

\begin{itemize}
  \item Data speed refers to the inflow and outflow speeds; inflow being the data that is being acquired, while outflow
  being the data leaving the system (often results of computations).
  \item Different classes of data often require different data speeds. \eg{}audio is often streamed at a far lesser speed
  than video, due to the relatively low amount of data in audio when compared with video.
\end{itemize}


\paragraph{Structure:}

\begin{itemize}
  \item Structure relates to whether data are in structured or unstructured formats.
  \item Generally unstructured data is more suitable for human consumption, such as literature or music.
  \item Structured data is usually structured in such a way that it is easily able to be parsed by an algorithm, often
  automated by computers.
  \item The structure of data directly relates to the difficulty of processing that data, as unstructured data usually
  will need some pre-processing or artificial intelligence to process.
\end{itemize}

G\'eczy later goes on to talk about the aspects of data that relate to data processing, similar to what will be talked
about later in~\sectref{sec:big_data_processing_background}.

Overall, G\'eczy looks at data characteristics, not from any particular perspective, but from one that attempts to capture
the interests and be relevant to a number of disciplines. This impartiality is a nice refreshment from most other literature
available on the topic, which have been shown to have been looking at data classification from a certain point-of-view.
However, this should not be misinterpreted as a criticism of the previous literature. It is simply that the classes of
data identified in other author's literature was more appropriate for the topic on which the rest of their research was
focussed on. Hence, the way they treated data changed accordingly. The paper presented by G\'eczy, simply titled
\emph{BIG DATA CHARACTERISTICS}, was focussed on nothing other than characteristics of data, hence there was no reason
to attempt to classify those characteristics based on any other domain-related biases. Additionally, G\'eczy's paper was
published in a notable interdisciplinary journal, rather than one aimed at a particular discipline. This difference in
terms of impartiality is the important difference to note with this paper.

% subsubsection characteristics_of_data_from_ge_czy (end)


\subsubsection{Criticisms of presented classification models} % (fold)
\label{ssub:criticisms_of_previously_presented_models}

From the literature presented previously in this section, there are a number of points to note. Firstly, they were all
highly varied in the classifications and characteristics of data given. As previously stated, this can be attributed
to the variety in sources for this literature; they were all published from quite different sources, and each of the
authors from different fields with different intentions. Hence, it is not

% TODO

% subsubsection criticisms_of_previously_presented_models (end)


% subsection data_classification (end)

% section big_data_types_background (end)


\section{Big data processing background} % (fold)
\label{sec:big_data_processing_background}

Much more work has been done in the area of data processing than the area related to classification of data; both in
the areas of big data and traditional data processing. Unlike data classification, which was more aimed at the classifying
of data in general, when looking at data processing, we are more interested in the relatively newer technologies which
enable the processing of big data, both in batch mode and realtime. Note that in this paper, we will refer to realtime
big data processing as just that; realtime data processing in the context of big data. Through use of this term, we encompass the meanings of
``data stream processing'', ``realtime stream processing'', and all other related terms that essentially have the meaning of:

% TODO: Place definition of realtime

\subsection{Batch data processing} % (fold)
\label{sub:batch_data_processing}

Over the last decade, the main ``go-to'' solution for any sort of processing needed on datasets falling under the
umbrella of big data has been the MapReduce programming model on top of some sort of scalable distributed storage
system~\cite{bifet_mining_2013}. From a very simplified functionality standpoint, the MapReduce programming model essentially
combines the common \textbf{Map} and \textbf{Reduce} functions (among others), found in the standard libraries of many functional
programming languages, such as Haskell~\cite{lammel2008google} or even Java 8~\cite{su2014changing}, to apply a specified
type of processing in a highly parallelised and distributed fashion~\cite{yang2007map}.

The MapReduce data processing model specialises in batch mode processing. Batch data processing can be thought of where
data needed to be processed is first queued up in batches before processing begins. Once ready, those batches get fed
into the processing system and handled accordingly. %TODO: try get a reference

\subsubsection{MapReduce and GFS} % (fold)
\label{ssub:mapreduce_and_gfs}

Dean and Ghemawat, in~\cite{dean_mapreduce:_2008}, originally presented MapReduce as a technology that had been
developed internally at Google, Inc.\ to be an abstraction to simplify the various computations that engineers were
trying to perform on their large datasets. The implementations of these computations, while not complicated functions
themselves, were obscured by the fact of having to manually parallelise the computations, distribute the data, and
handle faults all in an effective manner. The MapReduce model then enabled these computations to be expressed in a
simple, high-level manner without the programmer needing to worry about optimising for available resources. Furthermore,
the MapReduce abstraction provided high scalability to differently sized clusters.

As previously stated, the MapReduce programming model is generally used on top of some sort of distributed storage
system. In the previous case at Google, Inc., in the original MapReduce implementation, it was implemented on top of
their own proprietary distributed file system, known as Google File System (GFS). Ghemawat et al.,
in~\cite{ghemawat_google_2003}, define GFS to be a ``scalable distributed file system for large distributed data-intensive
applications'', noting that can be run on ``inexpensive commodity hardware''. Note that GFS was designed and in-use
at Google, Inc.\ years before they managed to develop their MapReduce abstraction, and the original paper on MapReduce
from Dean and Ghemawat state that GFS was used to manage data and store data from MapReduce~\cite{dean_mapreduce:_2008}.
Furthermore, McKusick and Quinlan, in~\cite{mckusick2009gfs}, state that, as of 2009, the majority of Google's data
relating to their many web-oriented applications are rely on GFS.

% subsubsection mapreduce_and_gfs (end)


\subsubsection{Hadoop MapReduce and HDFS} % (fold)
\label{ssub:hadoop_mapreduce_and_hdfs}

While MapReduce paired with GFS proved to be very successful solution for big data processing at Google, Inc., and
there was notable research published on the technology, it was proprietary in-house software unique to Google, and
availability elsewhere was often not an option~\cite{grossman2009varieties}. Hence, the open-source software community
responded in turn with their own implementation of MapReduce and a distributed file system analogous to GFS, known as the
Hadoop Distributed File System (HDFS). Both of these projects, along with others to date, make up the Apache Hadoop
big data framework~\footnote{https://hadoop.apache.org}. The Apache Hadoop framework, being a top level Apache Software
Foundation open source project, has been developed by a number of joint contributors from organisations and institutions
such as Yahoo!, Inc., Intel, IBM, UC Berkeley, among others~\cite{hadoop_committers}.

While Hadoop's MapReduce implementation very much was designed to be a functional replacements for Google's MapReduce,
HDFS is an entirely separate project in its own right. In the original paper from Yahoo!~\cite{shvachko2010hadoop},
Inc., Shvachko et al.\ present HDFS as ``the file system component of Hadoop'' with the intention of being similar to
the UNIX file system, however they also state that ``faithfulness to standards was sacrificed in favour of improved
performance''.

While HDFS was designed with replicating GFS' functionality in mind, several low-level architectural and design decisions
were made that substantially differ to those documented in GFS. For example, in~\cite{borthakur2007hadoop}, Borthakur
documents the method HDFS uses when it comes to file deletion. Borthakur talks about how when a file is deleted in HDFS,
it essentially gets moved to a \texttt{/trash} directory, much like what happens in a lot of modern operating systems.
This \texttt{/trash} directory is then purged after a configurable amount of time, the default of which being six hours.
To contrast with this, GFS is documented to have more primitive way of managing deleted files. Ghemawat, et al.,
in~\cite{ghemawat_google_2003}, document GFS' garbage collection implementation. Instead of having a centralised
\texttt{/trash} storage, deleted files get renamed to a hidden name. The GFS master then, during a regularly scheduled
scan, will delete any of these hidden files that have remained deleted for a configurable amount of time, the default
being three days. This is by far not the only difference between the two file systems, this is simply an example of a
less low-level technical difference.

% subsubsection hadoop_mapreduce_and_hdfs (end)


\subsubsection{Pig and Hive} % (fold)
\label{ssub:pig_and_hive}

Given the popularity of Hadoop, there were several early attempts at building further abstractions on top of the
MapReduce model, which were met with a high level of success. As highlighted earlier, MapReduce was originally designed
to be a nice abstraction on top of the underlying hardware, however according to Thusoo et al., in~\cite{thusoo2009hive},
MapReduce was still too low level resulting in programmers writing programs that are ``are hard to maintain and reuse''.
Thus, Thusoo et al.\ built the Hive abstraction on top of MapReduce. Hive allows programmers to write queries in a
similarly declarative language to SQL --- known affectionately as \emph{HiveQL} --- which then get compiled down into
MapReduce jobs to run on Hadoop~\cite{thusoo2010hive}.

Another common abstraction that was developed prior to Hive was what is known simply as Pig. Like Hive, Pig attempts to
be a further higher level abstraction on top of MapReduce, which ultimately compiles down into MapReduce jobs, although
what differentiates it from Hive is that instead of being a solely declarative SQL-like language, it is more of a mix of
procedural programming languages while allowing for SQL-like constraints to be specified on the data set to define the
result~\cite{olston2008pig}. Olston et al.\ describe Pig's language --- known as \emph{Pig Latin} --- to be what they
define as a ``dataflow language'', rather than a strictly procedural or declarative language.

Furthermore, note that Pig and Hive, being high level abstractions on top of MapReduce, also enable many of their own
optimisations to be applied to the underlying MapReduce jobs during the compilation
stage~\cite{gates2009building,thusoo2010hive} as well as having the benefit of being susceptible to manual query
optimisations, familiar to programmers familiar with query optimisations from SQL~\cite{gruenheid2011query}.

% subsubsection pig_and_hive (end)

% subsection batch_data_processing (end)


\subsection{Realtime data processing} % (fold)
\label{sub:realtime_data_processing}

With HDFS being an open source project with a large range of users~\cite{hadoop_users} and code
contributors~\cite{hadoop_committers}, it has grown as a project in the last few years for uses beyond what it was
originally intended for; a backend storage system for Hadoop MapReduce. HDFS is now not only used with Hadoop's
MapReduce but also with a variety of other technologies, a lot of which run as a part of the Hadoop ecosystem.
Big data processing has moved on from the more ``traditional'' method of processing, involving MapReduce jobs, which
were most suitable for batch processing of data, to those methods which specialise in the realtime processing of data.
The main difference of which is that rather than waiting for all the data before processing can be started, in realtime
data processing the data can be streamed into the processing system in realtime at any time in the whole process.

Comparing batched data processing to realtime data processing, it is useful to relate back to the four V's
identified in~\sectref{sub:four_v}. Velocity of data is often inconsistent with realtime processing, while in batch mode
processing, where you are processing the data that has already arrived and is waiting in batches to be processed, the
velocity can be considered consistent. Veracity of data is often not expected to be as consistent in realtime, as
sometimes there might be times where data does not arrive or only certain parts of the data arrive at certain times.
A realtime processing system, often called a data stream processing system (DSPS) in other literature, needs to be able
to deal with these timeliness issues, while a batch data processing system may expect everything that needs to be there
to be available.

\subsubsection{Hadoop YARN} % (fold)
\label{ssub:apache_hadoop_yarn_}

As previously looked at, the focus of the MapReduce model was performing distributed and highly parallel computations
on distributed batches of data. This suited a lot of the big data audience, and hence Hadoop became the dominant
method of big data processing~\cite{liu_survey_2014}. However for some more specialised applications, such as
the realtime monitoring of sensors, stock trading, and realtime web traffic analytics, the high latency between the data
arriving and actual results being generated from the computations was not satisfactory~\cite{kamburugamuve_survey_2014}.

A recent (2013) industry survey on European company use of big data technology by Bange, Grosser, and Janoschek, noted
in~\cite{industry_bd_survey}, shows that over 70\% of responders show a need for realtime processing. In that time,
there has certainly been a response from the open-source software community, responding with extensions to more
traditional batch systems, such as Hadoop, along with complete standalone DSPS solutions.

On the Hadoop front, the limitations of the MapReduce model were recognised, and a large effort was made in developing
the ``next generation'' of Hadoop so that it could be extensible and used with other programming models, not locked into
the rigidity of MapReduce. This became known officially known as YARN (Yet Another Resource Negotiator). According to
the original developers of YARN, Vavilapalli et al.\ state that YARN enables Hadoop to become more modular, decoupling
the resource management functionality of Hadoop from the programming model (traditionally, MapReduce)~\cite{vavilapalli2013apache}.
This decoupling essentially allowed for non-MapReduce technologies to be built on top of Hadoop, still interacting with the
overall ecosystem, allowing for much more flexible applications of big data processing on top of the existing robust
framework Hadoop provides.

Examples of such systems now built, or in some cases ported, to run on top of Hadoop, providing alternative processing
applications and use cases include:

\begin{itemize}
  \item Dryad, a general-purpose distributed execution system from Microsoft Research~\cite{isard2007dryad}. Dryad is
  aimed at being high level enough to make it ``easy'' for developers to write highly distributed and parallel applications.
  \item Spark, a data processing system, from researchers at UC Berkeley, that focuses on computations that reuse the same working data set over multiple
  parallel operations~\cite{zaharia2010spark}.
  \item HBase, a layer on top of a distributed file system (such as HDFS, or GFS) allowing for the storing of huge
  amounts of structured or semi-structured data~\cite{khetrapal2006hbase}.
\end{itemize}

These are just some of the more popular examples of applications built to interact with the Hadoop ecosystem via YARN.

% subsubsection apache_hadoop_yarn_ (end)


\subsubsection{Storm} % (fold)
\label{ssub:storm}

One very notable DSPS technology developed independently of Hadoop, and that is gaining immense popularity and growth in
user base, is the Storm project. Storm was originally developed by a team of engineers lead by Nathan Marz at
BackType~\footnote{https://storm.apache.org/}. BackType has since been acquired by Twitter, Inc.\ where development has
continued. Toshniwal et al.\ describe Storm, in the context of its use at Twitter, as ``a realtime distributed stream
data processing engine'' that ``powers the real-time stream data management tasks that are crucial to provide Twitter
services''~\cite{toshniwal2014storm}. Since the project's inception, Storm has seen massive adoption in industry,
including among some of the biggest names, such as Twitter, Yahoo!, Alibaba, and Baidu~\cite{storm_users}.

% subsubsection storm (end)

% subsection realtime_data_processing (end)

% section big_data_processing_background (end)


\section{Analysis and discussion} % (fold)
\label{sec:analysis_and_discussion}

% section analysis_and_discussion (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\newpage

\bibliographystyle{acm}
\bibliography{litreview.bib}

\newpage

\begin{appendices}
\renewcommand\thetable{\thesection\arabic{table}}
\renewcommand\thefigure{\thesection\arabic{figure}}
\section{IBM data classes} \label{app:ibm_data_classes}

\begin{table}[H]
\footnotesize
\centering
\hspace*{-2cm}
\begin{tabular}{ | p{2cm} | p{8cm} | p{7.5cm} | }
\hline
\textbf{Data class}          &  \textbf{Explanation} & \textbf{Characteristics}   \\ \hline

Machine generated data
&
\begin{itemize}[leftmargin=*]
  \item Data that is automatically generated as a by-product of some interaction with a machine.
  \item While Mysore et al.\ present this as being a distinct class in itself, it could be argued that this class
  is an umbrella class which many other data classes presented in their paper fall under. This will be touched upon
  further in later sections.
\end{itemize}
&
\begin{itemize}[leftmargin=*]
  \item Structured data (JSON, XML).
  \item Frequency of data varies depending on application.
\end{itemize}
\\ \hline

Web and social data
&
\begin{itemize}[leftmargin=*]
  \item Data that is automatically generated through use of the Internet or social media, such as Facebook or Twitter.
\end{itemize}
&
\begin{itemize}[leftmargin=*]
  \item Unstructured text (long: blogs, short: microblogs, Facebook).
  \item Miscellaneous multimedia (video, image, audio).
  \item On-demand frequency.
  \item Can be continuous feed of data in cases such as Twitter.
\end{itemize}
\\ \hline

Transaction data
&
\begin{itemize}[leftmargin=*]
  \item Data that is automatically generated as a by-product of transactions, such as money transactions or otherwise.
\end{itemize}
&
\begin{itemize}[leftmargin=*]
  \item Structured text (JSON, XML, logs).
  \item Continuous feed.
\end{itemize}
\\ \hline

Human generated data
&
\begin{itemize}[leftmargin=*]
  \item Data that is solely produced by humans.
  \item Examples of human generated data, as it is defined here, include such things as music, literature, recordings,
  and emails.
\end{itemize}
&
\begin{itemize}[leftmargin=*]
  \item Unstructured text (mail, literature).
  \item Miscellaneous multimedia (audio, video, images).
  \item Semi-structured text (email, online messaging services).
  \item On-demand frequency.
\end{itemize}
\\ \hline

 Biometrics data
 &
 \begin{itemize}[leftmargin=*]
   \item Data that relates to human bioinformatics.
 \end{itemize}
 &
 \begin{itemize}[leftmargin=*]
   \item Structured data.
   \item On-demand frequency.
   \item Continuous feeds of data in cases such as persistent health monitoring sensors (\ie{}hospital patients).
 \end{itemize}
 \\ \hline

\end{tabular}
\hspace*{5cm}
\end{table}

\newpage
\section{Mauris euismod}

\end{appendices}

\end{document}

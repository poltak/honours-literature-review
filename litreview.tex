\documentclass[a4paper,11pt]{article}

% set up sensible margins (same as for cssethesis)
\usepackage[paper=a4paper,left=30mm,width=150mm,top=25mm,bottom=25mm]{geometry}
\usepackage{setspace}               % This is used in the title page
\usepackage{graphicx}               % This is used to load the crest in the title page
\usepackage{poltakmacros}           % Personal macros included in file 'poltakmacros.sty'
\usepackage{enumitem}               % For nested enum lists
\usepackage[font={small}]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{url}


\author{Jonathan Poltak Samosir}
\title{Honours Research Proposal}

\begin{document}

% Set up a title page
\thispagestyle{empty} % no page number on very first page
% Use roman numerals for page numbers initially
\renewcommand{\thepage}{\roman{page}}

\begin{spacing}{1.5}
\begin{center}
{\Large \bfseries
Clayton School of Information Technology\\
Monash University}

\vspace*{30mm}

\includegraphics[width=5cm]{img/MonashCrest.pdf}

\vspace*{15mm}

{\large \bfseries
Honours Literature Review --- Semester 2, 2014
}

\vspace*{10mm}

{\LARGE \bfseries
A study of the Hadoop ecosystem for pipelined realtime data stream processing
}

\vspace*{20mm}

{\large \bfseries
Jonathan Poltak Samosir

[2271 3603]

\vspace*{20mm}

Supervisors: \parbox[t]{50mm}{\mbox{Dr Maria Indrawan-Santiago}\\Dr Pari Delir Haghighi}
}

\end{center}
\end{spacing}

\newpage

\tableofcontents

\newpage
% Now reset page number counter,and switch to arabic numerals for remaining page numbers
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}


% Start of content

\section{Introduction} % (fold)
\label{sec:introduction}

The realtime processing of big data is of great importance to both academia and industry. Advancements and progress in
modern society can be directly attributed back to data. The value of data has become more apparent, and data has become
a sort of currency for the information economy~\cite{st2009examining}. Hence, those in society who realised the value of
data early immense power over the entire economy and thus society overall~\cite{lievesley1993increasing}.
From seemingly inconsequential gains at the macro level, such as the ability to more
accurately predict the rise and fall of airline tickets~\cite{darlin2006airfares}, to those of utmost importance for
society as a whole, such as predicting and tracking the spread of the Swine Flu Pandemic in 2009 more accurately that
the United States Centers for Disease Control and Prevention could~\cite{ritterman2009using}~\cite{mayer2013big}. It is
example applications of big data processing like these that have been recognised by academics and organisations in
industry alike, with the last decade seeing a major shift in research and development into new methods for the handling
and processing of big data.

This paper will give a background on the types and classes of big data, as well as the various methods employed to
process those given classes of data. We will more specifically focussing on the methods that are involved with the
analysis and processing of realtime data streams, as opposed to the batch processing of big data. This paper will look
into detail at previous work that has been done in the field of big data, specifically those works that have had a
greater influence on the field  as a whole. This includes both works looking specifically at the processing of streaming
data, and works involving processed big data in batch mode, given that batch mode processing arguably led onto the
current hot-topic of realtime stream processing.

This paper will be structured in two main sections. In~\sectref{sec:big_data_types_background}, an overview of the different
classes and types of big data will be presented. This includes an overview of the big data classes presented through others' findings
as well as our own proposed classes for big data, based on the criticisms of those prior findings. In~\sectref{sec:big_data_processing_background},
an overview will be given of the major open-source big data processing systems. A special emphasis will be given on data stream processing
systems (DSPSs), given that the main area of this research is focusing on realtime data processing, or data stream processing.

\sectref{sec:relationships_between_big_data_classes_and_big_data_processing} will then give a discussion relating to future work
we have planned to form data processing recommendations based on the classification of specific data classes. All of the
sections will then be summarised in the conclusion in~\sectref{sec:conclusion}.

As a an outcome of this paper, we will identify a gap in previous research and development in the big data processing
field, upon which our future work will attempt to work towards filling.

% section introduction (end)


\section{Data types and characteristics background} % (fold)
\label{sec:big_data_types_background}

\subsection{Velocity, variety, volume, and veracity} % (fold)
\label{sub:four_v}

Data, and more specifically, big data, are often characterised into what is known as the ``four V's''~\cite{wang2014bigdatabench}.
These can be thought of as different ``dimensions'' of big data, and can be summarised as follows~\cite{dong2013big}:

\begin{itemize}
  \item \emph{Velocity:} The rate at which data is being collected and made available to the data consumers.
  \item \emph{Variety:} The heterogeneity of data. Big data often exhibits substantial variations in both the structural
  level and the instance level (representations of real-world entities). This is often highlighted by data systems that
  depend on acquiring of data from a number of non-conforming, and sometimes unrelated, data sources.
  \item \emph{Volume:} The amount of data that is obtained by the data consumer from the data source/s.
  \item \emph{Veracity:} The quality, in terms of accuracy, coverage, and timeliness, of data that is consumed from
  the data source/s. Veracity of data can widely differ between sources.
\end{itemize}

While the four V's are often described in terms of big data, they can also apply in general to more traditional data
warehousing and processing, albeit on a far smaller scale. In the domain of big data processing, data will
exhibit signs of high velocity, variety, and volume~\cite{beyer2011gartner}, and hence the veracity of the data may also fluctuate. Meanwhile,
in more traditional data processing, the scope may be limited, especially in terms of factors such as variety and, as a
consequence, there is less need of an emphasis on veracity due to limited variety in data sources.

% subsection velocity_variety_volume_and_veracity (end)

\subsection{Classification of data} % (fold)
\label{sub:data_classification}

Data, in general, can be categorised into a number of different classes or types. In this paper, we will define the
concept of a data class to mean the same as the terms of ``data type'', ``data category'', or ``data format'', as all terms were often
used interchangeably in other literature.

Each class of data can be further defined and categorised via the characteristics they exhibit. Furthermore, these
characteristics exhibited by data classes can be exploited and it is often possible to optimise the processing of each
class of data by processing it using a specific method depending on those characteristics.%TODO cite
To give an example of this, data that is expected to have highly iterative
processing applied to it would benefit from a data processor that does not have to unnecessarily write to disk after
every single iteration. The elimination of this I/O overhead is an example of the optimisations that could be applied to
the overall process from correctly identifying the data class beforehand, and processing it accordingly.

Furthermore, particular classes of data are generally only found in particular applications or use cases of data
processing.%TODO cite
As this is the case, it narrows down the amount of classification needed, depending on the application that
is being looked at. This will be elaborated on in later parts of this section.

There is no concrete, universally accepted standard for the classification of data. While the study of big data processing
could arguably be considered still in its infancy (or at least temperamental toddler stage), data handling and processing
in general is relatively mature. From preliminary research on looking at past work and literature in this area,
it must be noted that in the there is a significant lack of research on the classification of data. The literature that
will be reviewed in this section is often not wholly focused on the idea of data classification, hence data classification
is presented relative to whatever the overall topic of the literature is on. This is important to note, as one attempt
at data classification may not be appropriate under a different context. This also explains the large variation in different
classification attempts, although we will also highlight the recurring similarities between different data classification
literature.

The main piece of literature that this section sources is a white paper from IBM Architects Mysore, Khupat, and Jain,
published by IBM in 2013~\cite{ibm_big_2013}. The white paper is targeted towards beginners in the area of big data
processing; much like the set of recommendations that we intend to produce from this research project. The paper looks
at identifying the different data classes, or ``formats'' as they were labelled in the paper, that are commonly
encountered in big data. For each of these formats, what was identified was the underlying characteristics of the data,
and it was noted that the type of processing needed would be dependent on those characteristics.


\subsubsection{Data characteristics, from Mysore et al.} % (fold)
\label{ssub:data_charact}

The characteristics of data, as put forward by Mysore et al., in~\cite{ibm_big_2013}, include the following:
\par
\textbf{Analysis type:}

\begin{itemize}
  \item Whether or not the data would be processed/analysed in realtime, or batched for later processing.
  \item Often this data class characteristic is dependent on the application of the data (\eg{}The processing of social
  media data for the analysis of currently occurring events would want to be processed in realtime, regardless of the
  type of data that is involved).
\end{itemize}

\textbf{Processing methodology:}

\begin{itemize}
  \item This characteristic involves the approach used when processing the data.
  \item Some examples of different processing methodologies include: predictive processing, analytical, ad-hoc queries,
  and reporting.
  \item Often the processing methodology for a particular class is determined by the business requirements or application
  of the data.
  \item Depending on the processing methodology used, many different combinations of big data technologies can be used.
\end{itemize}

\textbf{Data frequency and size:}

\begin{itemize}
  \item The amount of data expected to arrive to the processing system, along with the speed and regularity of the incoming data.
  \item Knowing this characteristic beforehand can determine the methods for data storage and preprocessing, if needed.
  \item Examples of data frequency includes: on-demand data (social media), continuous/realtime (weather data, transactions),
  time-series (email).
  \item Considering the four V's, the characteristic of data frequency and size directly relates back to velocity and volume.
\end{itemize}

\textbf{Content format:}

\begin{itemize}
  \item This characteristic relates back to the structure of the underlying data.
  \item Examples of data content format include: structured (JSON, XML), unstructured (human-readable literature),
  semi-structured (email).
\end{itemize}

\textbf{Data source:}

\begin{itemize}
  \item This characteristic relates back to where the data originated from.
  \item As discussed previously in~\sectref{sub:four_v}, the origin of data can have a great effect on whether or not
  that data is usable, as data often varies greatly, especially when many different sources are used which may or may
  not conform to a specific content format.
  \item Another thing that is dependent on the data source is whether or not the data can be trusted.
  \item Considering the four V's, the characteristic of data source directly relates back to veracity and variety.
\end{itemize}


% subsubsection data_charact (end)

 \subsubsection{Data classifications, from Mysore et al.} % (fold)
 \label{ssub:data_classification}

The following table will highlight the different classes of data put forward by Mysore, et al., in~\cite{ibm_big_2013}.
The identified characteristics for each class of data will also be given.

\hspace*{-3cm}
\begingroup
\fontsize{8pt}{10pt}\selectfont
\begin{tabular}{ | p{1.5cm} | p{8cm} | p{8cm} | }
  \hline
  \textbf{Data class}          &  \textbf{Explanation} & \textbf{Characteristics}   \\ \hline

  Machine generated data
  &
  \begin{itemize}
    \item Data that is automatically generated as a by-product of some interaction with a machine.
    \item While Mysore et al.\ present this as being a distinct class in itself, it could be argued that this class
    is an umbrella class which many other data classes presented in their paper fall under. This will be touched upon
    further in later sections.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured data (JSON, XML).
    \item Frequency of data varies depending on application.
  \end{itemize}
  \\ \hline

  Web and social data
  &
  \begin{itemize}
    \item Data that is automatically generated through use of the Internet or social media, such as Facebook or Twitter.
  \end{itemize}
  &
  \begin{itemize}
    \item Unstructured text (long: blogs, short: microblogs, Facebook).
    \item Miscellaneous multimedia (video, image, audio).
    \item On-demand frequency.
    \item Can be continuous feed of data in cases such as Twitter.
  \end{itemize}
  \\ \hline

  Transaction data
  &
  \begin{itemize}
    \item Data that is automatically generated as a by-product of transactions, such as money transactions or otherwise.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured text (JSON, XML, logs).
    \item Continuous feed.
  \end{itemize}
  \\ \hline

  Human generated data
  &
  \begin{itemize}
    \item Data that is solely produced by humans.
    \item Examples of human generated data, as it is defined here, include such things as music, literature, recordings,
    and emails.
  \end{itemize}
  &
  \begin{itemize}
    \item Unstructured text (mail, literature).
    \item Miscellaneous multimedia (audio, video, images).
    \item Semi-structured text (email, online messaging services).
    \item On-demand frequency.
  \end{itemize}
  \\ \hline

  Biometrics data
  &
  \begin{itemize}
    \item Data that relates to human bioinformatics.
  \end{itemize}
  &
  \begin{itemize}
    \item Structured data.
    \item On-demand frequency.
    \item Continuous feeds of data in cases such as persistent health monitoring sensors (\ie{}hospital patients).
  \end{itemize}
  \\ \hline

\end{tabular}
\endgroup
\hspace*{-3cm}

The classes and characteristics of data presented by Mysore et al., in~\cite{ibm_big_2013}, are highly oriented towards
industry and business users, coming from an IBM-published paper. While this is not an issue as such, as noted earlier
in this section, these characteristics and data classes are defined within the domain relevant to this paper. As such,
they may not be as relevant or appropriate for usage in other, non-business domains.

% subsubsection data_classification (end)


\subsubsection{Characteristics of data, from G\'eczy} % (fold)
\label{ssub:characteristics_of_data_from_ge_czy}

G\'eczy attempts to characterise

% subsubsection characteristics_of_data_from_ge_czy (end)

% subsection data_classification (end)

% section big_data_types_background (end)


\section{Big data processing background} % (fold)
\label{sec:big_data_processing_background}

Much work has been done in the area of big data processing. As discussed in

% section big_data_processing_background (end)


\section{Relationships between big data classes and big data processing} % (fold)
\label{sec:relationships_between_big_data_classes_and_big_data_processing}

% section relationships_between_big_data_classes_and_big_data_processing (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\newpage

\bibliographystyle{acm}
\bibliography{litreview.bib}

\end{document}
